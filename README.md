# ğŸ¨ XPU Ray Stable Diffusion Service

A high-performance Stable Diffusion service powered by Intel XPU and Ray Serve, supporting multiple models with authentication and load balancing.

## âœ¨ Features

- **ğŸ–¼ï¸ Multiple Model Support**:
  - Stable Diffusion 2.0 (SD2)
  - Stable Diffusion XL (SDXL)
  - SDXL-Turbo
  - SDXL-Lightning

- **âš¡ Intel XPU Optimization**:
  - Optimized for Intel GPUs using Intel Extension for PyTorch
  - Efficient memory management
  - Hardware-accelerated inference

- **ğŸš€ Production-Ready Features**:
  - ğŸ” Token-based authentication
  - âš–ï¸ Load balancing with Traefik
  - ğŸ¥ Health checks and monitoring
  - ğŸ¤– Automatic model management
  - ğŸ“Š Request queuing and rate limiting

## ğŸ“‹ Prerequisites

- ğŸ³ Docker and Docker Compose
- ğŸ® Intel GPU with appropriate drivers
- ğŸ’¾ 32GB+ RAM recommended
- ğŸ§ Ubuntu 22.04 or later

## ğŸš€ Quick Start

1. Clone the repository:
```bash
git clone https://github.com/rahulunair/xpu_ray.git
cd xpu_ray
```

2. Deploy the service:
```bash
./deploy.sh
```

The script will:
- ğŸ”‘ Generate an authentication token
- ğŸš€ Start all services
- â³ Wait for models to load
- ğŸ“ Display the API endpoint and token

## ğŸ”Œ API Endpoints

### ğŸ¨ Generate Image
```bash
curl -X POST "http://localhost:8000/imagine/sdxl-turbo" \
     -H "Authorization: Bearer $VALID_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
       "prompt": "a beautiful sunset over mountains",
       "img_size": 1024
     }'
```

### ğŸ’“ Check Service Health
```bash
curl http://localhost:8000/health
```

### â„¹ï¸ Get Model Information
```bash
curl http://localhost:8000/info
```

### ğŸ”„ Reload Specific Model
```bash
curl -X POST "http://localhost:8000/reload_model/sdxl-turbo" \
     -H "Authorization: Bearer $VALID_TOKEN"
```

## âš™ï¸ Model Configurations

| Model | Steps | Guidance | Min Size | Max Size |
|-------|--------|-----------|-----------|-----------|
| SD2 | 50 | 7.5 | 512 | 768 |
| SDXL | 50 | 7.5 | 512 | 1024 |
| SDXL-Turbo | 1 | 0.0 | 512 | 1024 |
| SDXL-Lightning | 4 | 0.0 | 512 | 1024 |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚â”€â”€â”€â”€â–¶â”‚ Traefik  â”‚â”€â”€â”€â”€â–¶â”‚    Auth     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Ray Serve   â”‚
                  â”‚ SD Service  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Management Commands

### ğŸš€ Start Services
```bash
docker compose up -d
```

### ğŸ›‘ Stop Services
```bash
docker compose down
```

### ğŸ§¹ Clean Up
```bash
docker compose down --remove-orphans
docker rmi xpu_ray-sd-service xpu_ray-auth xpu_ray-traefik
```

### ğŸ“‹ View Logs
```bash
docker compose logs -f
```

## ğŸ”§ Environment Variables

- `VALID_TOKEN`: Authentication token
- `DIFFUSERS_CACHE`: Cache directory for diffusers
- `HF_HOME`: Hugging Face home directory

## ğŸ“ˆ Performance Considerations

- ğŸ”„ Models are loaded on demand
- ğŸ§¹ Memory is cleared after each generation
- ğŸ’“ Health checks monitor system resources
- ğŸ“Š Request queuing prevents overload

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch
3. Commit your changes
4. Push to the branch
5. Create a new Pull Request

## ğŸ“„ License

[Your License Here]

## ğŸ™ Acknowledgments

- Intel Extension for PyTorch
- Hugging Face Diffusers
- Ray Project
- Stability AI